{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"ExoDoc\" - Deep Learning for Tumor Detection and Identification in Exo Travellers\n",
    "\n",
    "ExoDoc is a CNN (Convolutional Neural Network) created to detect and classify brain tumors in MRI scans of the pioneering spacefarers of the 22nd century. It is built using Tensorflow and Keras, and has been trained on 3000 MRI scans of space explorers to detect 4 distinct classes of tumors.\n",
    "\n",
    "ExoDoc was built by Caramel Labs as a submission for the Datathon of the Tech-Triathlon, organized by Rootcode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 575,
     "status": "ok",
     "timestamp": 1692883989956,
     "user": {
      "displayName": "Manujaya Sri",
      "userId": "03268972178084293049"
     },
     "user_tz": -330
    },
    "id": "xJ3yxbintIzR",
    "outputId": "1cf8879c-223c-402e-8cd1-23c29572bcd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world!\n"
     ]
    }
   ],
   "source": [
    "# Test Python runtime\n",
    "\n",
    "print('Hello world!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 3684,
     "status": "ok",
     "timestamp": 1692888514956,
     "user": {
      "displayName": "Manujaya Sri",
      "userId": "03268972178084293049"
     },
     "user_tz": -330
    },
    "id": "rZRlm5v3tifJ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-25 04:12:18.797243: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-25 04:12:18.803138: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-25 04:12:18.897590: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-25 04:12:18.899193: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-25 04:12:21.593720: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow imports\n",
    "\n",
    "# 1. Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# 2. Neural network architecture\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D, BatchNormalization, Activation\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1316,
     "status": "ok",
     "timestamp": 1692888520555,
     "user": {
      "displayName": "Manujaya Sri",
      "userId": "03268972178084293049"
     },
     "user_tz": -330
    },
    "id": "SMDSv5oJuglJ"
   },
   "outputs": [],
   "source": [
    "# Other imports\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "Our dataset is a collection of 3000 brain MRI scans, each being 512 x 512 px. They have been taken from various angles and portray various depths of the human brain. The dataset is composed of 4 distinct classes:\n",
    "\n",
    "1. Category 1 tumor\n",
    "2. Category 2 tumor\n",
    "3. Category 3 tumor\n",
    "4. No tumor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the current file structure of the data looks like:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<project-root>\n",
    "    - data\n",
    "        - category1_tumor\n",
    "            img1.jpg\n",
    "            img2.jpg\n",
    "            img3.jpg\n",
    "            ...\n",
    "        - category2_tumor\n",
    "            img1.jpg\n",
    "            img2.jpg\n",
    "            img3.jpg\n",
    "            ...\n",
    "        - category3_tumor\n",
    "            img1.jpg\n",
    "            img2.jpg\n",
    "            img3.jpg\n",
    "            ...\n",
    "        - no_tumor\n",
    "            img1.jpg\n",
    "            img2.jpg\n",
    "            img3.jpg\n",
    "            ...\n",
    "    jupyter-notebook.ipynb\n",
    "    [other-files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating the images into training and testing datasets will be beneficial for us. We can execute a simple Python script to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 494,
     "status": "ok",
     "timestamp": 1692888533047,
     "user": {
      "displayName": "Manujaya Sri",
      "userId": "03268972178084293049"
     },
     "user_tz": -330
    },
    "id": "P5COrbc2uh90"
   },
   "outputs": [],
   "source": [
    "# Automatically move images into training and testing folders\n",
    "\n",
    "def split_data_into_folders():\n",
    "    # Path to original dataset folder\n",
    "    original_data_path = './data'\n",
    "\n",
    "    # Path to create the new train and test folders\n",
    "    train_data_path = './data/train'\n",
    "    test_data_path = './data/test'\n",
    "\n",
    "    # Create train and test folders if they don't exist\n",
    "    os.makedirs(train_data_path, exist_ok=True)\n",
    "    os.makedirs(test_data_path, exist_ok=True)\n",
    "\n",
    "    # List of category folders\n",
    "    categories = ['category1_tumor', 'category2_tumor', 'category3_tumor', 'no_tumor']\n",
    "\n",
    "    # Iterate through each category\n",
    "    for category in categories:\n",
    "        category_path = os.path.join(original_data_path, category)\n",
    "        images = os.listdir(category_path)\n",
    "        train_images, test_images = train_test_split(images, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Create subdirectories for each category in train and validation folders\n",
    "        os.makedirs(os.path.join(train_data_path, category), exist_ok=True)\n",
    "        os.makedirs(os.path.join(test_data_path, category), exist_ok=True)\n",
    "\n",
    "        # Move images to train and validation folders\n",
    "        for img in train_images:\n",
    "            src = os.path.join(category_path, img)\n",
    "            dest = os.path.join(train_data_path, category, img)\n",
    "            shutil.copy(src, dest)\n",
    "\n",
    "        for img in test_images:\n",
    "            src = os.path.join(category_path, img)\n",
    "            dest = os.path.join(test_data_path, category, img)\n",
    "            shutil.copy(src, dest)\n",
    "\n",
    "split_data_into_folders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data now looks like:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<project-root>\n",
    "    - data\n",
    "        - train\n",
    "            - category1_tumor\n",
    "                img1.jpg\n",
    "                img2.jpg\n",
    "                img3.jpg\n",
    "                ...\n",
    "            - category2_tumor\n",
    "                img1.jpg\n",
    "                img2.jpg\n",
    "                img3.jpg\n",
    "                ...\n",
    "            - category3_tumor\n",
    "                img1.jpg\n",
    "                img2.jpg\n",
    "                img3.jpg\n",
    "                ...\n",
    "            - no_tumor\n",
    "                img1.jpg\n",
    "                img2.jpg\n",
    "                img3.jpg\n",
    "                ...\n",
    "        - test\n",
    "            - category1_tumor\n",
    "                img1.jpg\n",
    "                img2.jpg\n",
    "                img3.jpg\n",
    "                ...\n",
    "            - category2_tumor\n",
    "                img1.jpg\n",
    "                img2.jpg\n",
    "                img3.jpg\n",
    "                ...\n",
    "            - category3_tumor\n",
    "                img1.jpg\n",
    "                img2.jpg\n",
    "                img3.jpg\n",
    "                ...\n",
    "            - no_tumor\n",
    "                img1.jpg\n",
    "                img2.jpg\n",
    "                img3.jpg\n",
    "                ...\n",
    "    jupyter-notebook.ipynb\n",
    "    [other-files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have a relatively small dataset (in the scale of datasets usually used to train neural networks), we will augment the dataset with `ImageDataGenerator`. This allows the model to receive new variations of its training images for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0va40b1UupP6"
   },
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Instantiate ImageDataGenerator with augmentation settings\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # Normalize pixel values to [0, 1]\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the `flow_from_directory()` method of `ImageDataGenerator` to automatically label the data based on the folder structure we created previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1232,
     "status": "ok",
     "timestamp": 1692866213015,
     "user": {
      "displayName": "Ravindu Aratchige",
      "userId": "14880698477337757419"
     },
     "user_tz": -330
    },
    "id": "scbM9UQTuuOu",
    "outputId": "d120d653-1a92-4079-ba4e-0784983993b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2577 images belonging to 4 classes.\n",
      "Found 647 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data loading and preprocessing\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    './data/train',\n",
    "    target_size=(256, 256),  # Resize images as necessary\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = datagen.flow_from_directory(\n",
    "    './data/test',\n",
    "    target_size=(256, 256),  # Resize images as necessary\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MayYGcWhuyvG"
   },
   "source": [
    "## The Neural Network\n",
    "\n",
    "It is common knowledge within the circles of ML enthusiasts that CNNs (Convolutional Neural Networks) are renown for their performance in image analysis. So, we decided to build ExoDoc using a custom-built CNN (instead of pre-trained as they are not permitted by the rules of the Datathon). After multiple iterations, endless research papers and countless hours on Kaggle, using inspiration from the architectures of models created by other ML practitioners, we built our CNN using the following architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "umEBTAcbuxvS"
   },
   "outputs": [],
   "source": [
    "# Defining the model architecture\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 3), padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Dense(4, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 717,
     "status": "ok",
     "timestamp": 1692864980544,
     "user": {
      "displayName": "Ravindu Aratchige",
      "userId": "14880698477337757419"
     },
     "user_tz": -330
    },
    "id": "Xksp7nD-UioM",
    "outputId": "8dfa0c60-5b53-4234-a29d-7884ff281ce3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 256, 256, 32)      896       \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 256, 256, 32)      128       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " activation (Activation)     (None, 256, 256, 32)      0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 256, 256, 32)      9248      \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 256, 256, 32)      128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 256, 256, 32)      0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 128, 128, 32)      0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128, 128, 32)      0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 128, 128, 64)      18496     \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 128, 128, 64)      256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 128, 128, 64)      0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 128, 128, 64)      36928     \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 128, 128, 64)      256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 128, 128, 64)      0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 64, 64, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64, 64, 64)        0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 64, 64, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 64, 64, 128)       512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 64, 64, 128)       0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 64, 64, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 64, 64, 128)       512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 64, 64, 128)       0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 32, 32, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 32, 32, 128)       0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 32, 32, 256)       295168    \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 32, 32, 256)       1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 32, 32, 256)       0         \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 32, 32, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 32, 32, 256)       1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 32, 32, 256)       0         \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 16, 16, 256)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 16, 16, 256)       0         \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 16, 16, 512)       1180160   \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 16, 16, 512)       2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 16, 16, 512)       0         \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 16, 16, 512)       2359808   \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, 16, 16, 512)       2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 16, 16, 512)       0         \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 8, 8, 512)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 8, 8, 512)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 512)               0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               65664     \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4)                 132       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4796292 (18.30 MB)\n",
      "Trainable params: 4792324 (18.28 MB)\n",
      "Non-trainable params: 3968 (15.50 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Getting a summary of the model\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use callbacks to hook into various stages of the training process to ensure that overfitting is minimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "KI2GyY1YL09-"
   },
   "outputs": [],
   "source": [
    "# Defining callbacks\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "early_stop = EarlyStopping(monitor='loss',\n",
    "                           patience=5,\n",
    "                           verbose = 1)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.2,\n",
    "                              patience=5,\n",
    "                              verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "KFve0kTFu3nc"
   },
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After compiling the model, we can start training. We selected a relatively high number of epochs to reach the highest possible accuracy before reaching a plateau. We also included the previously defined callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qHYsj81FyyMM",
    "outputId": "66aa31a5-528a-4132-e502-f4f74923c5fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "80/80 [==============================] - 158s 2s/step - loss: 1.3583 - accuracy: 0.3399 - val_loss: 1.3722 - val_accuracy: 0.2781 - lr: 0.0010\n",
      "Epoch 2/40\n",
      "80/80 [==============================] - 153s 2s/step - loss: 1.1624 - accuracy: 0.4483 - val_loss: 1.3668 - val_accuracy: 0.2766 - lr: 0.0010\n",
      "Epoch 3/40\n",
      "80/80 [==============================] - 152s 2s/step - loss: 1.1045 - accuracy: 0.5053 - val_loss: 1.3636 - val_accuracy: 0.2844 - lr: 0.0010\n",
      "Epoch 4/40\n",
      "80/80 [==============================] - 151s 2s/step - loss: 1.0272 - accuracy: 0.5532 - val_loss: 1.3678 - val_accuracy: 0.3344 - lr: 0.0010\n",
      "Epoch 5/40\n",
      "80/80 [==============================] - 152s 2s/step - loss: 0.9859 - accuracy: 0.5859 - val_loss: 1.9402 - val_accuracy: 0.3359 - lr: 0.0010\n",
      "Epoch 6/40\n",
      "80/80 [==============================] - 151s 2s/step - loss: 0.9116 - accuracy: 0.6322 - val_loss: 2.4327 - val_accuracy: 0.3359 - lr: 0.0010\n",
      "Epoch 7/40\n",
      "80/80 [==============================] - 151s 2s/step - loss: 0.8286 - accuracy: 0.6864 - val_loss: 3.3357 - val_accuracy: 0.3297 - lr: 0.0010\n",
      "Epoch 8/40\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.7864 - accuracy: 0.7151\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "80/80 [==============================] - 153s 2s/step - loss: 0.7864 - accuracy: 0.7151 - val_loss: 2.5423 - val_accuracy: 0.4156 - lr: 0.0010\n",
      "Epoch 9/40\n",
      "80/80 [==============================] - 152s 2s/step - loss: 0.6936 - accuracy: 0.7367 - val_loss: 0.8745 - val_accuracy: 0.6594 - lr: 2.0000e-04\n",
      "Epoch 10/40\n",
      "80/80 [==============================] - 151s 2s/step - loss: 0.6504 - accuracy: 0.7580 - val_loss: 1.0319 - val_accuracy: 0.6141 - lr: 2.0000e-04\n",
      "Epoch 11/40\n",
      "80/80 [==============================] - 152s 2s/step - loss: 0.6439 - accuracy: 0.7682 - val_loss: 0.8421 - val_accuracy: 0.6844 - lr: 2.0000e-04\n",
      "Epoch 12/40\n",
      "80/80 [==============================] - 152s 2s/step - loss: 0.5783 - accuracy: 0.7882 - val_loss: 0.6933 - val_accuracy: 0.7422 - lr: 2.0000e-04\n",
      "Epoch 13/40\n",
      "80/80 [==============================] - 151s 2s/step - loss: 0.6000 - accuracy: 0.7839 - val_loss: 0.7740 - val_accuracy: 0.7109 - lr: 2.0000e-04\n",
      "Epoch 14/40\n",
      "80/80 [==============================] - 152s 2s/step - loss: 0.5633 - accuracy: 0.8047 - val_loss: 0.7467 - val_accuracy: 0.7031 - lr: 2.0000e-04\n",
      "Epoch 15/40\n",
      "80/80 [==============================] - 152s 2s/step - loss: 0.5470 - accuracy: 0.8012 - val_loss: 0.5421 - val_accuracy: 0.7984 - lr: 2.0000e-04\n",
      "Epoch 16/40\n",
      "80/80 [==============================] - 151s 2s/step - loss: 0.5366 - accuracy: 0.8075 - val_loss: 0.5968 - val_accuracy: 0.7828 - lr: 2.0000e-04\n",
      "Epoch 17/40\n",
      "80/80 [==============================] - 151s 2s/step - loss: 0.5122 - accuracy: 0.8177 - val_loss: 0.7063 - val_accuracy: 0.7422 - lr: 2.0000e-04\n",
      "Epoch 18/40\n",
      "80/80 [==============================] - 151s 2s/step - loss: 0.5182 - accuracy: 0.8141 - val_loss: 1.2153 - val_accuracy: 0.6406 - lr: 2.0000e-04\n",
      "Epoch 19/40\n",
      "80/80 [==============================] - 152s 2s/step - loss: 0.4960 - accuracy: 0.8295 - val_loss: 0.5293 - val_accuracy: 0.7969 - lr: 2.0000e-04\n",
      "Epoch 20/40\n",
      "80/80 [==============================] - 151s 2s/step - loss: 0.4950 - accuracy: 0.8381 - val_loss: 0.5791 - val_accuracy: 0.7984 - lr: 2.0000e-04\n",
      "Epoch 21/40\n",
      "80/80 [==============================] - 152s 2s/step - loss: 0.4629 - accuracy: 0.8413 - val_loss: 0.4388 - val_accuracy: 0.8453 - lr: 2.0000e-04\n",
      "Epoch 22/40\n",
      "80/80 [==============================] - 151s 2s/step - loss: 0.4709 - accuracy: 0.8299 - val_loss: 0.6495 - val_accuracy: 0.7641 - lr: 2.0000e-04\n",
      "Epoch 23/40\n",
      "80/80 [==============================] - 151s 2s/step - loss: 0.4157 - accuracy: 0.8515 - val_loss: 0.7082 - val_accuracy: 0.7188 - lr: 2.0000e-04\n",
      "Epoch 24/40\n",
      "80/80 [==============================] - 150s 2s/step - loss: 0.4636 - accuracy: 0.8310 - val_loss: 0.7403 - val_accuracy: 0.7281 - lr: 2.0000e-04\n",
      "Epoch 25/40\n",
      "80/80 [==============================] - 150s 2s/step - loss: 0.4418 - accuracy: 0.8456 - val_loss: 0.8080 - val_accuracy: 0.7047 - lr: 2.0000e-04\n",
      "Epoch 26/40\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.4398 - accuracy: 0.8503\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "80/80 [==============================] - 151s 2s/step - loss: 0.4398 - accuracy: 0.8503 - val_loss: 0.8211 - val_accuracy: 0.7578 - lr: 2.0000e-04\n",
      "Epoch 27/40\n",
      "80/80 [==============================] - 151s 2s/step - loss: 0.4041 - accuracy: 0.8617 - val_loss: 0.5167 - val_accuracy: 0.8172 - lr: 4.0000e-05\n",
      "Epoch 28/40\n",
      "80/80 [==============================] - 150s 2s/step - loss: 0.3841 - accuracy: 0.8676 - val_loss: 0.4991 - val_accuracy: 0.8281 - lr: 4.0000e-05\n",
      "Epoch 29/40\n",
      "80/80 [==============================] - 150s 2s/step - loss: 0.3646 - accuracy: 0.8743 - val_loss: 0.3931 - val_accuracy: 0.8578 - lr: 4.0000e-05\n",
      "Epoch 30/40\n",
      "80/80 [==============================] - 150s 2s/step - loss: 0.3792 - accuracy: 0.8727 - val_loss: 0.3445 - val_accuracy: 0.8750 - lr: 4.0000e-05\n",
      "Epoch 31/40\n",
      "80/80 [==============================] - 150s 2s/step - loss: 0.3565 - accuracy: 0.8782 - val_loss: 0.4505 - val_accuracy: 0.8531 - lr: 4.0000e-05\n",
      "Epoch 32/40\n",
      "80/80 [==============================] - 150s 2s/step - loss: 0.3451 - accuracy: 0.8778 - val_loss: 0.3388 - val_accuracy: 0.8625 - lr: 4.0000e-05\n",
      "Epoch 33/40\n",
      "80/80 [==============================] - 150s 2s/step - loss: 0.3450 - accuracy: 0.8813 - val_loss: 0.4143 - val_accuracy: 0.8516 - lr: 4.0000e-05\n",
      "Epoch 34/40\n",
      "80/80 [==============================] - 151s 2s/step - loss: 0.3539 - accuracy: 0.8782 - val_loss: 0.3838 - val_accuracy: 0.8672 - lr: 4.0000e-05\n",
      "Epoch 35/40\n",
      "80/80 [==============================] - 151s 2s/step - loss: 0.3305 - accuracy: 0.8861 - val_loss: 0.3468 - val_accuracy: 0.8687 - lr: 4.0000e-05\n",
      "Epoch 36/40\n",
      "80/80 [==============================] - 151s 2s/step - loss: 0.3941 - accuracy: 0.8707 - val_loss: 0.3627 - val_accuracy: 0.8531 - lr: 4.0000e-05\n",
      "Epoch 37/40\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.3297 - accuracy: 0.8884\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "80/80 [==============================] - 150s 2s/step - loss: 0.3297 - accuracy: 0.8884 - val_loss: 0.3407 - val_accuracy: 0.8797 - lr: 4.0000e-05\n",
      "Epoch 38/40\n",
      "80/80 [==============================] - 151s 2s/step - loss: 0.3250 - accuracy: 0.8876 - val_loss: 0.3311 - val_accuracy: 0.8891 - lr: 8.0000e-06\n",
      "Epoch 39/40\n",
      "80/80 [==============================] - 151s 2s/step - loss: 0.3120 - accuracy: 0.8884 - val_loss: 0.3186 - val_accuracy: 0.8844 - lr: 8.0000e-06\n",
      "Epoch 40/40\n",
      "80/80 [==============================] - 150s 2s/step - loss: 0.3172 - accuracy: 0.8998 - val_loss: 0.3262 - val_accuracy: 0.8734 - lr: 8.0000e-06\n"
     ]
    }
   ],
   "source": [
    "# Training the Neural Network\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=40,  # Adjust the number of epochs as needed\n",
    "    validation_data=test_generator,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    validation_steps=test_generator.samples // batch_size,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can check how well our model performs against testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 85570,
     "status": "ok",
     "timestamp": 1692864212788,
     "user": {
      "displayName": "Ravindu Aratchige",
      "userId": "14880698477337757419"
     },
     "user_tz": -330
    },
    "id": "7TEIH8s1y7z0",
    "outputId": "7aa11bc5-3919-46a9-b419-860c77ffcb97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 15s 695ms/step - loss: 0.3472 - accuracy: 0.8748\n",
      "Test loss:  0.34719160199165344\n",
      "Test accuracy:  0.874806821346283\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the loss\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_generator, verbose=1)\n",
    "print(\"Test loss: \", test_loss)\n",
    "print(\"Test accuracy: \", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final accuracy is a satisfactory 87.48%. However, we will continue to improve this by further hyperparameter fine-tuning and rethinking the model's architecture.\n",
    "\n",
    "We can now export our model to be used in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IUJEIA7rzC16"
   },
   "outputs": [],
   "source": [
    "# Serializing the model\n",
    "\n",
    "model.save('mri_classification_model_v3.h5')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
